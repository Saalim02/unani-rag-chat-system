### Problem Statement : 
-Large surgery textbooks are difficult to search efficiently during exam preparation. 
This project uses a RAG-based LLM system to retrieve accurate, textbook-grounded answers for medical students.

### ğŸ Python
- Acts as the **core programming language**
- Used to glue all components together:
  - text processing
  - model inference
  - retrieval logic
  - UI integration

---

### ğŸ“„ PDF / Text Processing Libraries
- Used to extract raw text from the **SRS Surgery book (PDF)**
- Convert book content into plain text so it can be processed by NLP tools

ğŸ“Œ Without this step, the book cannot be used by machine learning models.

---

### âœ‚ï¸ Regex & Basic NLP
- Used to clean and structure unstructured medical text
- Helps in:
  - removing page numbers, headers, footers
  - fixing broken sentences and spacing
  - identifying section headings and content blocks

ğŸ“Œ This step converts **messy textbook text into clean, usable data**.

---

### ğŸ§© Text Chunking Logic
- Splits cleaned text into **small, overlapping chunks**
- Prevents loss of context during retrieval
- Ensures the LLM gets only relevant portions of the book

ğŸ“Œ Chunking is critical for accurate RAG systems.

---

### ğŸ¤— Hugging Face Transformers
- Used to load **Transformer-based models**
- Provides:
  - embedding models (sentence transformers)
  - tokenizer support
  - LLM inference (depending on setup)

ğŸ“Œ Transformers convert text into **dense numerical vectors** that capture meaning.

---

### ğŸ”¢ Embeddings
- Each text chunk is converted into a numerical vector (embedding)
- Similar text produces similar vectors
- Enables semantic search instead of keyword search

ğŸ“Œ Embeddings allow the system to â€œunderstand meaningâ€, not just words.

---

### ğŸ“¦ FAISS (Facebook AI Similarity Search)
- Stores all embeddings in a **vector database**
- Performs fast similarity search when a user asks a question
- Retrieves the most relevant textbook sections

ğŸ“Œ FAISS is the **retrieval engine** of the RAG system.

---

### ğŸ”— LangChain
- Acts as the **orchestration framework**
- Connects:
  - user query
  - embedding model
  - FAISS retriever
  - transformer-based language model
- Ensures answers are generated **only from retrieved content**

ğŸ“Œ LangChain controls the RAG workflow end-to-end.

---

### ğŸ§  Transformer-based Language Model (LLM)
- Generates human-readable answers
- Uses **only the retrieved textbook content**
- Does not invent or hallucinate new medical facts

ğŸ“Œ The LLM is responsible for **answer formulation**, not knowledge storage.

---

### {(ğŸ–¥ï¸ TO be deployed in Streamlit )}
- Used to build a simple and interactive **web interface**
- Allows users to:
  - type medical questions
  - view chatbot responses
- Makes the system usable by **non-technical medical students**

ğŸ“Œ Streamlit turns the backend RAG system into a usable application.

## How Everything Works Together (One-Line Summary)
PDF Text â†’ Regex Cleaning â†’ Chunking â†’ Transformer Embeddings â†’ FAISS Retrieval â†’ LangChain RAG â†’ LLM Answer â†’  ( to be deployed in Streamlit UI )

